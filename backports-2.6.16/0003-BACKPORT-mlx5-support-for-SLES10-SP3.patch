From: Eugenia Emantayev <eugenia@mellanox.com>
Subject: [PATCH] BACKPORT: mlx5 support for SLES10 SP3

Change-Id: I952cabe29c7025e262e3eaa19727e4efdb0e9692
Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
---
 drivers/infiniband/hw/mlx5/doorbell.c              |   8 +
 drivers/infiniband/hw/mlx5/main.c                  | 167 ++++++++++++++++++++-
 drivers/infiniband/hw/mlx5/mem.c                   |  96 ++++++++++++
 drivers/infiniband/hw/mlx5/mlx5_ib.h               |   5 +-
 drivers/infiniband/hw/mlx5/mr.c                    |  78 ++++++++++
 drivers/infiniband/hw/mlx5/qp.c                    |  17 +++
 drivers/infiniband/hw/mlx5/roce.c                  |   2 +
 drivers/net/ethernet/mellanox/mlx5/core/Makefile   |   6 +-
 drivers/net/ethernet/mellanox/mlx5/core/alloc.c    |  11 ++
 drivers/net/ethernet/mellanox/mlx5/core/cmd.c      |  16 ++
 drivers/net/ethernet/mellanox/mlx5/core/debugfs.c  |  19 +++
 drivers/net/ethernet/mellanox/mlx5/core/main.c     |  43 +++++-
 .../net/ethernet/mellanox/mlx5/core/mlx5_core.h    |   9 ++
 .../net/ethernet/mellanox/mlx5/core/pagealloc.c    |   4 +
 drivers/net/ethernet/mellanox/mlx5/core/uar.c      |   2 +
 drivers/net/ethernet/mellanox/mlx5/core/vport.c    |   3 +-
 include/linux/clocksource.h                        |   3 +-
 include/linux/mlx5/driver.h                        |  12 +-
 18 files changed, 488 insertions(+), 13 deletions(-)

--- a/drivers/infiniband/hw/mlx5/doorbell.c
+++ b/drivers/infiniband/hw/mlx5/doorbell.c
@@ -47,6 +47,9 @@ int mlx5_ib_db_map_user(struct mlx5_ib_u
 			struct mlx5_db *db)
 {
 	struct mlx5_ib_user_db_page *page;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	struct ib_umem_chunk *chunk;
+#endif
 	int err = 0;
 
 	mutex_lock(&context->db_page_mutex);
@@ -74,7 +77,12 @@ int mlx5_ib_db_map_user(struct mlx5_ib_u
 	list_add(&page->list, &context->db_page_list);
 
 found:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	db->dma = sg_dma_address(page->umem->sg_head.sgl) + (virt & ~PAGE_MASK);
+#else
+	chunk = list_entry(page->umem->chunk_list.next, struct ib_umem_chunk, list);
+	db->dma		= sg_dma_address(chunk->page_list) + (virt & ~PAGE_MASK);
+#endif
 	db->u.user_page = page;
 	++page->refcnt;
 
--- a/drivers/infiniband/hw/mlx5/main.c
+++ b/drivers/infiniband/hw/mlx5/main.c
@@ -36,7 +36,9 @@
 #include <linux/pci.h>
 #include <linux/dma-mapping.h>
 #include <linux/slab.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/io-mapping.h>
+#endif
 #include <linux/sched.h>
 #include <linux/highmem.h>
 #include <linux/spinlock.h>
@@ -73,6 +75,15 @@ static char mlx5_version[] =
 	DRIVER_NAME ": Mellanox Connect-IB Infiniband driver v"
 	DRIVER_VERSION " (" DRIVER_RELDATE ")\n";
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+#define MLX5_WC_FLAGS   (_PAGE_PWT)
+
+pgprot_t pgprot_wc(pgprot_t _prot)
+{
+	return __pgprot(pgprot_val(_prot) | MLX5_WC_FLAGS);
+}
+#endif
+
 static void ext_atomic_caps(struct mlx5_ib_dev *dev,
 			    struct ib_exp_device_attr *props)
 {
@@ -239,6 +250,7 @@ static int mlx5_query_system_image_guid(
 		return err;
 
 	case MLX5_VPORT_ACCESS_METHOD_NIC:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (!MLX5_CAP_GEN(dev->mdev, roce)) {
 			mlx5_ib_warn(dev, "Trying to query system image GUID "
 				     "but RoCE is not supported\n");
@@ -248,7 +260,9 @@ static int mlx5_query_system_image_guid(
 		if (!err)
 			*sys_image_guid = cpu_to_be64(tmp);
 		return err;
-
+#else
+		return 0;
+#endif
 	default:
 		return -EINVAL;
 	}
@@ -316,6 +330,7 @@ static int mlx5_query_node_guid(struct m
 		return err;
 
 	case MLX5_VPORT_ACCESS_METHOD_NIC:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if (!MLX5_CAP_GEN(dev->mdev, roce)) {
 			mlx5_ib_warn(dev, "Trying to query node GUID but RoCE "
 				     "is not supported\n");
@@ -326,7 +341,9 @@ static int mlx5_query_node_guid(struct m
 			*node_guid = cpu_to_be64(tmp);
 
 		return err;
-
+#else
+		return 0;
+#endif
 	default:
 		return -EINVAL;
 	}
@@ -424,7 +441,11 @@ static int query_device(struct ib_device
 	}
 
 	props->vendor_part_id	   = mdev->pdev->device;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	props->hw_ver		   = mdev->pdev->revision;
+#else
+	pci_read_config_byte(mdev->pdev, PCI_REVISION_ID, &mdev->rev_id);
+#endif
 
 	props->max_mr_size	   = ~0ull;
 	props->page_size_cap	   = ~(u32)((1ull << MLX5_CAP_GEN(mdev, log_pg_sz)) -1);
@@ -997,7 +1018,11 @@ static int mlx5_ib_dealloc_ucontext(stru
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static phys_addr_t uar_index2pfn(struct mlx5_ib_dev *dev, int index)
+#else
+static u64 uar_index2pfn(struct mlx5_ib_dev *dev, int index)
+#endif
 {
 	return (pci_resource_start(dev->mdev->pdev, 0) >> PAGE_SHIFT) + index;
 }
@@ -1142,7 +1167,11 @@ static inline bool mlx5_writecombine_ava
 {
 	pgprot_t prot = __pgprot(0);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (pgprot_val(pgprot_writecombine(prot)) == pgprot_val(pgprot_noncached(prot)))
+#else
+	if (pgprot_val(pgprot_wc(prot)) == pgprot_val(pgprot_noncached(prot)))
+#endif
 		return false;
 
 	return true;
@@ -1153,7 +1182,11 @@ static int uar_mmap(struct vm_area_struc
 		    struct mlx5_ib_ucontext *context)
 {
 	unsigned long idx;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	phys_addr_t pfn;
+#else
+       u64 pfn;
+#endif
 	struct mlx5_ib_vma_private_data *vma_prv;
 
 	if (vma->vm_end - vma->vm_start != PAGE_SIZE) {
@@ -1261,15 +1294,18 @@ static int mlx5_ib_mmap(struct ib_uconte
 	struct mlx5_dc_tracer *dct;
 	unsigned long command;
 	int err;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	unsigned long total_size;
 	unsigned long order;
 	struct ib_cmem *ib_cmem;
 	int numa_node;
+#endif
 	phys_addr_t pfn;
 
 	command = get_command(vma->vm_pgoff);
 	switch (command) {
 	case MLX5_IB_MMAP_MAP_DC_INFO_PAGE:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		if ((MLX5_CAP_GEN(dev->mdev, port_type) !=
 		    MLX5_CAP_PORT_TYPE_IB) || (!mlx5_core_is_pf(dev->mdev)))
 			return -ENOTSUPP;
@@ -1287,9 +1323,15 @@ static int mlx5_ib_mmap(struct ib_uconte
 			return err;
 		}
 		break;
-
+#else
+		return -ENOTSUPP;
+#endif
 	case MLX5_IB_MMAP_REGULAR_PAGE:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		return uar_mmap(vma, pgprot_writecombine(vma->vm_page_prot),
+#else
+		return uar_mmap(vma, pgprot_wc(vma->vm_page_prot),
+#endif
 				mlx5_writecombine_available(),
 				uuari, dev, context);
 
@@ -1297,6 +1339,7 @@ static int mlx5_ib_mmap(struct ib_uconte
 
 	case MLX5_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_CPU_NUMA:
 	case MLX5_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_DEV_NUMA:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	case MLX5_IB_MMAP_GET_CONTIGUOUS_PAGES:
 		if (command == MLX5_IB_EXP_MMAP_GET_CONTIGUOUS_PAGES_CPU_NUMA)
 			numa_node = numa_node_id();
@@ -1328,7 +1371,11 @@ static int mlx5_ib_mmap(struct ib_uconte
 			return -EPERM;
 		}
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		return uar_mmap(vma, pgprot_writecombine(vma->vm_page_prot),
+#else
+		return uar_mmap(vma, pgprot_wc(vma->vm_page_prot),
+#endif
 				true, uuari, dev, context);
 		break;
 
@@ -1340,6 +1387,7 @@ static int mlx5_ib_mmap(struct ib_uconte
 	case MLX5_IB_EXP_ALLOC_N_MMAP_WC:
 		return alloc_and_map_wc(dev, context, get_index(vma->vm_pgoff),
 					vma);
+#endif
 		break;
 
 	case MLX5_IB_EXP_MMAP_CORE_CLOCK:
@@ -2180,11 +2228,16 @@ static int init_node_data(struct mlx5_ib
 	if (err)
 		return err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	dev->mdev->rev_id = dev->mdev->pdev->revision;
+#else
+	pci_read_config_byte(dev->mdev->pdev, PCI_REVISION_ID, &dev->mdev->rev_id);
+#endif
 
 	return mlx5_query_node_guid(dev, &dev->ib_dev.node_guid);
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static ssize_t show_fw_pages(struct device *device, struct device_attribute *attr,
 			     char *buf)
 {
@@ -2252,6 +2305,74 @@ static struct device_attribute *mlx5_cla
 	&dev_attr_fw_pages,
 	&dev_attr_reg_pages,
 };
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+
+static ssize_t show_fw_pages(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%d\n", dev->mdev->priv.fw_pages);
+}
+
+static ssize_t show_reg_pages(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%d\n", dev->mdev->priv.reg_pages);
+}
+
+static ssize_t show_hca(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "MT%d\n", dev->mdev->pdev->device);
+}
+
+static ssize_t show_fw_ver(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%d.%d.%d\n", fw_rev_maj(dev->mdev),
+		       fw_rev_min(dev->mdev), fw_rev_sub(dev->mdev));
+}
+
+static ssize_t show_rev(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%x\n", dev->mdev->rev_id);
+}
+
+static ssize_t show_board(struct class_device *cdev, char *buf)
+{
+	struct mlx5_ib_dev *dev = container_of(cdev, struct mlx5_ib_dev,
+					       ib_dev.class_dev);
+
+	return sprintf(buf, "%.*s\n", MLX5_BOARD_ID_LEN,
+		       dev->mdev->board_id);
+}
+
+static CLASS_DEVICE_ATTR(hw_rev,   S_IRUGO, show_rev,    NULL);
+static CLASS_DEVICE_ATTR(fw_ver,   S_IRUGO, show_fw_ver, NULL);
+static CLASS_DEVICE_ATTR(hca_type, S_IRUGO, show_hca,    NULL);
+static CLASS_DEVICE_ATTR(board_id, S_IRUGO, show_board,  NULL);
+static CLASS_DEVICE_ATTR(fw_pages, S_IRUGO, show_fw_pages, NULL);
+static CLASS_DEVICE_ATTR(reg_pages, S_IRUGO, show_reg_pages, NULL);
+
+static struct class_device_attribute *mlx5_class_attributes[] = {
+	&class_device_attr_hw_rev,
+	&class_device_attr_fw_ver,
+	&class_device_attr_hca_type,
+	&class_device_attr_board_id,
+	&class_device_attr_fw_pages,
+	&class_device_attr_reg_pages,
+};
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 
 static void mlx5_ib_handle_internal_error(struct mlx5_ib_dev *ibdev)
 {
@@ -2969,7 +3090,11 @@ static void enable_dc_tracer(struct mlx5
 	dct->size = size;
 	dct->order = order;
 	dct->dma = dma_map_page(device, dct->pg, 0, size, DMA_FROM_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(device, dct->dma)) {
+#else
+	if (dma_mapping_error(dct->dma)) {
+#endif
 		mlx5_ib_err(dev, "dma mapping error\n");
 		goto map_err;
 	}
@@ -3456,7 +3581,11 @@ static struct kobj_type dc_type = {
 
 static int init_sysfs(struct mlx5_ib_dev *dev)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct device *device = &dev->ib_dev.dev;
+#else
+	struct class_device *device = &dev->ib_dev.class_dev;
+#endif
 
 	dev->dc_kobj = kobject_create_and_add("dct", &device->kobj);
 	if (!dev->dc_kobj) {
@@ -3470,7 +3599,11 @@ static int init_sysfs(struct mlx5_ib_dev
 static void cleanup_sysfs(struct mlx5_ib_dev *dev)
 {
 	if (dev->dc_kobj) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		kobject_put(dev->dc_kobj);
+#else
+		kobject_unregister(dev->dc_kobj);
+#endif
 		dev->dc_kobj = NULL;
 	}
 }
@@ -3886,7 +4019,11 @@ static void destroy_ports_attrs(struct m
 	}
 
 	if (dev->ports_parent) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		kobject_put(dev->ports_parent);
+#else
+		kobject_unregister(dev->ports_parent);
+#endif
 		dev->ports_parent = NULL;
 	}
 }
@@ -3895,7 +4032,11 @@ static int create_port_attrs(struct mlx5
 {
 	int ret = 0;
 	unsigned int i = 0;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct device *device = &dev->ib_dev.dev;
+#else
+	struct class_device *device = &dev->ib_dev.class_dev;
+#endif
 
 	dev->ports_parent = kobject_create_and_add("mlx5_ports",
 						   &device->kobj);
@@ -3954,6 +4095,7 @@ static void *mlx5_ib_add(struct mlx5_cor
 
 	if (mlx5_use_mad_ifc(dev))
 		get_ext_port_caps(dev);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mlx5_ib_port_link_layer(&dev->ib_dev, 1) ==
 	    IB_LINK_LAYER_ETHERNET) {
 		if (MLX5_CAP_GEN(mdev, roce)) {
@@ -3965,6 +4107,7 @@ static void *mlx5_ib_add(struct mlx5_cor
 		}
 	}
 
+#endif
 	MLX5_INIT_DOORBELL_LOCK(&dev->uar_lock);
 
 	strlcpy(dev->ib_dev.name, "mlx5_%d", IB_DEVICE_NAME_MAX);
@@ -4151,22 +4294,32 @@ static void *mlx5_ib_add(struct mlx5_cor
 	if (err)
 		goto err_dev;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (MLX5_CAP_GEN(dev->mdev, port_type) ==
 	    MLX5_CAP_PORT_TYPE_IB) {
 		if (init_dc_improvements(dev))
 			mlx5_ib_dbg(dev, "init_dc_improvements - continuing\n");
 	}
+#endif
 
 	err = create_port_attrs(dev);
 	if (err)
 		goto err_dc;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	for (i = 0; i < ARRAY_SIZE(mlx5_class_attributes); i++) {
 		err = device_create_file(&dev->ib_dev.dev,
 					 mlx5_class_attributes[i]);
 		if (err)
 			goto err_port_attrs;
 	}
+#else
+	for (i = 0; i < ARRAY_SIZE(mlx5_class_attributes); ++i) {
+		if (class_device_create_file(&dev->ib_dev.class_dev,
+				       mlx5_class_attributes[i]))
+			goto err_dc;
+	}
+#endif
 
 	dev->ib_active = true;
 
@@ -4176,9 +4329,11 @@ err_port_attrs:
 	destroy_ports_attrs(dev, dev->num_ports);
 
 err_dc:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (MLX5_CAP_GEN(dev->mdev, port_type) ==
 	    MLX5_CAP_PORT_TYPE_IB)
 		cleanup_dc_improvements(dev);
+#endif
 	destroy_umrc_res(dev);
 
 err_dev:
@@ -4194,9 +4349,11 @@ err_rsrc:
 	destroy_dev_resources(&dev->devr);
 
 err_disable_roce:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mlx5_ib_port_link_layer(&dev->ib_dev, 1) ==
 	    IB_LINK_LAYER_ETHERNET && MLX5_CAP_GEN(mdev, roce))
 		mlx5_nic_vport_disable_roce(mdev);
+#endif
 err_free_port:
 	kfree(dev->port);
 
@@ -4217,18 +4374,22 @@ static void mlx5_ib_remove(struct mlx5_c
 	int i;
 
 	destroy_ports_attrs(dev, dev->num_ports);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (MLX5_CAP_GEN(dev->mdev, port_type) ==
 	    MLX5_CAP_PORT_TYPE_IB)
 		cleanup_dc_improvements(dev);
+#endif
 	mlx5_ib_dealloc_q_counters(dev);
 	ib_unregister_device(&dev->ib_dev);
 	destroy_umrc_res(dev);
 	mlx5_ib_odp_remove_one(dev);
 	destroy_dev_resources(&dev->devr);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mlx5_ib_port_link_layer(&dev->ib_dev, 1) ==
 	    IB_LINK_LAYER_ETHERNET && MLX5_CAP_GEN(mdev, roce))
 		mlx5_nic_vport_disable_roce(mdev);
+#endif
 
 	for (i = 0; i < MLX5_CAP_GEN(mdev, num_ports); i++)
 		if (mlx5_ib_port_link_layer(&dev->ib_dev, i + 1) ==
--- a/drivers/infiniband/hw/mlx5/mem.c
+++ b/drivers/infiniband/hw/mlx5/mem.c
@@ -48,6 +48,7 @@ void mlx5_ib_cont_pages(struct ib_umem *
 			int *count, int *shift,
 			int *ncont, int *order)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	unsigned long tmp;
 	unsigned long m;
 	int i, k;
@@ -124,6 +125,71 @@ void mlx5_ib_cont_pages(struct ib_umem *
 	}
 	*shift = page_shift + m;
 	*count = i;
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+	struct ib_umem_chunk *chunk;
+	int i, j, k;
+	u64 len;
+	u64 pfn;
+	u64 base = 0;
+	unsigned long m;
+	int skip;
+	int mask;
+	int p = 0;
+	unsigned long tmp;
+
+	addr = addr >> PAGE_SHIFT;
+	tmp = (unsigned long)addr;
+	m = find_first_bit(&tmp, sizeof(tmp));
+	if (max_page_shift)
+		m = min_t(unsigned long, max_page_shift - PAGE_SHIFT, m);
+	skip = 1 << m;
+	mask = skip - 1;
+	i = 0;
+	list_for_each_entry(chunk, &umem->chunk_list, list)
+		for (j = 0; j < chunk->nmap; ++j) {
+			len = sg_dma_len(&chunk->page_list[j]) >> PAGE_SHIFT;
+			pfn = sg_dma_address(&chunk->page_list[j]) >> PAGE_SHIFT;
+			for (k = 0; k < len; ++k) {
+				if (!(i & mask)) {
+					tmp = (unsigned long)pfn;
+					m = min(m, find_first_bit(&tmp, sizeof(tmp)));
+					skip = 1 << m;
+					mask = skip - 1;
+					base = pfn;
+					p = 0;
+				} else {
+					if (base + p != pfn) {
+						tmp = (unsigned long)p;
+						m = find_first_bit(&tmp, sizeof(tmp));
+						skip = 1 << m;
+						mask = skip - 1;
+						base = pfn;
+						p = 0;
+					}
+				}
+				++p;
+				++i;
+			}
+		}
+
+	if (i) {
+		m = min_t(unsigned long, ilog2(roundup_pow_of_two(i)), m);
+
+		if (order)
+			*order = ilog2(roundup_pow_of_two(i) >> m);
+
+		*ncont = DIV_ROUND_UP(i, (1 << m));
+	} else {
+		m  = 0;
+
+		if (order)
+			*order = 0;
+
+		*ncont = 0;
+	}
+	*shift = PAGE_SHIFT + m;
+	*count = i;
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 }
 
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
@@ -157,6 +223,7 @@ void __mlx5_ib_populate_pas(struct mlx5_
 			    int page_shift, size_t offset, size_t num_pages,
 			    __be64 *pas, int access_flags)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	unsigned long umem_page_shift = ilog2(umem->page_size);
 	int shift = page_shift - umem_page_shift;
 	int mask = (1 << shift) - 1;
@@ -200,6 +267,35 @@ void __mlx5_ib_populate_pas(struct mlx5_
 			i++;
 		}
 	}
+#else /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
+	struct ib_umem_chunk *chunk;
+	int i, j, k;
+	int len;
+	u64 cur = 0;
+	u64 base;
+	int shift = page_shift - PAGE_SHIFT;
+	int mask = (1 << shift) - 1;
+
+	i = 0;
+	list_for_each_entry(chunk, &umem->chunk_list, list)
+		for (j = 0; j < chunk->nmap; ++j) {
+			len = sg_dma_len(&chunk->page_list[j]) >> PAGE_SHIFT;
+			base = sg_dma_address(&chunk->page_list[j]);
+			for (k = 0; k < len; ++k) {
+				if (!(i & mask)) {
+					cur = base + (k << PAGE_SHIFT);
+					cur |= access_flags;
+
+					pas[i >> shift] = cpu_to_be64(cur);
+					mlx5_ib_dbg(dev, "pas[%d] 0x%llx\n",
+						    i >> shift, be64_to_cpu(pas[i >> shift]));
+				}  else
+					mlx5_ib_dbg(dev, "=====> 0x%llx\n",
+						    base + (k << PAGE_SHIFT));
+				++i;
+			}
+		}
+#endif /* LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16) */
 }
 
 void mlx5_ib_populate_pas(struct mlx5_ib_dev *dev, struct ib_umem *umem,
--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h
+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h
@@ -1036,8 +1036,11 @@ static inline u8 convert_access(int acc)
 	       (acc & IB_ACCESS_LOCAL_WRITE   ? MLX5_PERM_LOCAL_WRITE  : 0) |
 	       MLX5_PERM_LOCAL_READ;
 }
-
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #define MLX5_MAX_UMR_SHIFT 16
+#else
+#define MLX5_MAX_UMR_SHIFT 14
+#endif
 #define MLX5_MAX_UMR_PAGES (1 << MLX5_MAX_UMR_SHIFT)
 
 #endif /* MLX5_IB_H */
--- a/drivers/infiniband/hw/mlx5/mr.c
+++ b/drivers/infiniband/hw/mlx5/mr.c
@@ -45,9 +45,11 @@
 #include <rdma/ib_umem_odp.h>
 #include <rdma/ib_verbs.h>
 #include "mlx5_ib.h"
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void mlx5_invalidate_umem(void *invalidation_cookie,
 				 struct ib_umem *umem,
 				 unsigned long addr, size_t size);
+#endif
 
 enum {
 	MAX_PENDING_REG_MR = 8,
@@ -697,7 +699,11 @@ static struct mlx5_ib_mr *reg_umr(struct
 	memset(pas + npages, 0, size - npages * sizeof(u64));
 
 	dma = dma_map_single(ddev, pas, size, DMA_TO_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(ddev, dma)) {
+#else
+	if (dma_mapping_error(mr->dma)) {
+#endif
 		mlx5_ib_err(dev, "dma mapping failed\n");
 		err = -ENOMEM;
 		goto free_pas;
@@ -795,7 +801,11 @@ int mlx5_ib_update_mtt(struct mlx5_ib_mr
 	}
 	pages_iter = size / sizeof(u64);
 	dma = dma_map_single(ddev, pas, size, DMA_TO_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(ddev, dma)) {
+#else
+	if (dma_mapping_error(mr->dma)) {
+#endif
 		mlx5_ib_err(dev, "unable to map DMA during MTT update.\n");
 		err = -ENOMEM;
 		goto free_pas;
@@ -1267,7 +1277,11 @@ static struct mlx5_ib_mr *reg_klm(struct
 	}
 
 	dma = dma_map_single(ddev, dptr, dsize, DMA_TO_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(ddev, dma)) {
+#else
+	if (dma_mapping_error(dma)) {
+#endif
 		err = -ENOMEM;
 		mlx5_ib_warn(dev, "dma map failed\n");
 		goto out;
@@ -1396,7 +1410,9 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 	int ncont;
 	int order;
 	int err;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct ib_peer_memory_client *ib_peer_mem;
+#endif
 
 	if (access_flags & IB_ACCESS_PHYSICAL_ADDR) {
 #ifdef CONFIG_INFINIBAND_PA_MR
@@ -1412,13 +1428,20 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 
 	mlx5_ib_dbg(dev, "start 0x%llx, virt_addr 0x%llx, length 0x%llx, access_flags 0x%x\n",
 		    start, virt_addr, length, access_flags);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	umem = ib_umem_get_ex(pd->uobject->context, start, length, access_flags,
 			      0, 1);
+#else
+	umem = ib_umem_get(pd->uobject->context, start, length, access_flags,
+			   0);
+#endif
 	if (IS_ERR(umem)) {
 		mlx5_ib_warn(dev, "umem get failed (%ld)\n", PTR_ERR(umem));
 		return (void *)umem;
 	}
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	ib_peer_mem = umem->ib_peer_mem;
+#endif
 
 	mlx5_ib_cont_pages(umem, start, MLX5_MKEY_PAGE_SHIFT_MASK,
 			   &npages, &page_shift, &ncont, &order);
@@ -1472,6 +1495,7 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 	atomic_add(npages, &dev->mdev->priv.reg_pages);
 	mr->ibmr.lkey = mr->mmr.key;
 	mr->ibmr.rkey = mr->mmr.key;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	atomic_set(&mr->invalidated, 0);
 
 	if (ib_peer_mem) {
@@ -1479,6 +1503,8 @@ struct ib_mr *mlx5_ib_reg_user_mr(struct
 		ib_umem_activate_invalidation_notifier(umem,
 					mlx5_invalidate_umem, mr);
 	}
+#endif
+
 #ifdef CONFIG_INFINIBAND_ON_DEMAND_PAGING
 	if (umem->odp_data) {
 		/*
@@ -1594,6 +1620,7 @@ int mlx5_ib_dereg_mr(struct ib_mr *ibmr)
 	return 0;
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void mlx5_invalidate_umem(void *invalidation_cookie,
 				 struct ib_umem *umem,
 				 unsigned long addr, size_t size)
@@ -1614,6 +1641,7 @@ out:
 
 
 }
+#endif
 
 static int create_mr_sig(struct ib_pd *pd,
 			 struct ib_mr_init_attr *mr_init_attr,
@@ -1946,7 +1974,11 @@ static ssize_t limit_store(struct cache_
 	u32 var;
 	int err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var > ent->size)
@@ -1983,7 +2015,11 @@ static ssize_t miss_store(struct cache_o
 	struct mlx5_cache_ent *ent = &cache->ent[co->index];
 	u32 var;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var != 0)
@@ -2015,7 +2051,11 @@ static ssize_t size_store(struct cache_o
 	u32 var;
 	int err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var < ent->limit)
@@ -2027,7 +2067,11 @@ static ssize_t size_store(struct cache_o
 			if (err && err != -EAGAIN)
 				return err;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 			usleep_range(3000, 5000);
+#else
+			msleep(4);
+#endif
 		} while (err);
 	} else if (var < ent->size) {
 		remove_keys(dev, co->index, ent->size - var);
@@ -2118,7 +2162,11 @@ static ssize_t rel_imm_store(struct mlx5
 	int i;
 	int found = 0;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtouint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%u", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var > 1)
@@ -2156,7 +2204,11 @@ static ssize_t rel_timeout_store(struct
 	int var;
 	int i;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (kstrtoint(buf, 0, &var))
+#else
+	if (sscanf(buf, "%d", &var) != 1)
+#endif
 		return -EINVAL;
 
 	if (var < -1 || var > MAX_MR_RELEASE_TIMEOUT)
@@ -2233,7 +2285,11 @@ static struct kobj_type cache_type = {
 static int mlx5_mr_sysfs_init(struct mlx5_ib_dev *dev)
 {
 	struct mlx5_mr_cache *cache = &dev->cache;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	struct device *device = &dev->ib_dev.dev;
+#else
+	struct class_device *device = &dev->ib_dev.class_dev;
+#endif
 	struct cache_order *co;
 	int o;
 	int i;
@@ -2262,9 +2318,18 @@ static int mlx5_mr_sysfs_init(struct mlx
 err_put:
 	for (; i >= 0; i--) {
 		co = &cache->ent[i].co;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		kobject_put(&co->kobj);
+#else
+		kobject_unregister(&co->kobj);
+#endif
 	}
+
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	kobject_put(&dev->mr_cache);
+#else
+	kobject_unregister(&dev->mr_cache);
+#endif
 
 	return err;
 }
@@ -2277,9 +2342,17 @@ static void mlx5_mr_sysfs_cleanup(struct
 
 	for (i = MAX_MR_CACHE_ENTRIES - 1; i >= 0; i--) {
 		co = &cache->ent[i].co;
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		kobject_put(&co->kobj);
+#else
+		kobject_unregister(&co->kobj);
+#endif
 	}
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	kobject_put(&dev->mr_cache);
+#else
+	kobject_unregister(&dev->mr_cache);
+#endif
 }
 
 int mlx5_ib_exp_query_mkey(struct ib_mr *mr, u64 mkey_attr_mask,
@@ -2316,6 +2389,7 @@ mlx5_ib_alloc_indir_reg_list(struct ib_d
 #ifdef ARCH_KMALLOC_MINALIGN
 	dsize += max_t(int, MLX5_UMR_ALIGN - ARCH_KMALLOC_MINALIGN, 0);
 #else
+#define CRYPTO_MINALIGN __alignof__(unsigned long long)
 	dsize += max_t(int, MLX5_UMR_ALIGN - CRYPTO_MINALIGN, 0);
 #endif
 	mirl->mapped_ilist = kzalloc(dsize, GFP_KERNEL);
@@ -2328,7 +2402,11 @@ mlx5_ib_alloc_indir_reg_list(struct ib_d
 			      MLX5_UMR_ALIGN);
 	mirl->map = dma_map_single(ddev, mirl->klms,
 				   dsize, DMA_TO_DEVICE);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (dma_mapping_error(ddev, mirl->map)) {
+#else
+	if (dma_mapping_error(mirl->map)) {
+#endif
 		err = -ENOMEM;
 		goto err_dma_map;
 	}
--- a/drivers/infiniband/hw/mlx5/qp.c
+++ b/drivers/infiniband/hw/mlx5/qp.c
@@ -140,6 +140,7 @@ void *mlx5_get_send_wqe(struct mlx5_ib_q
  *
  * Return: the number of bytes copied, or an error code.
  */
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 int mlx5_ib_read_user_wqe(struct mlx5_ib_qp *qp, int send, int wqe_index,
 			  void *buffer, u32 length)
 {
@@ -193,6 +194,7 @@ int mlx5_ib_read_user_wqe(struct mlx5_ib
 
 	return wqe_length;
 }
+#endif
 
 static int
 query_wqe_idx(struct mlx5_ib_qp *qp)
@@ -2071,6 +2073,7 @@ static struct mlx5_ib_pd *get_pd(struct
 	return to_mpd(qp->ibqp.pd);
 }
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 static void destroy_raw_qp_rules(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp)
 {
 	struct mlx5_ib_fs_mc_flow *flow_iter;
@@ -2088,10 +2091,13 @@ static void destroy_raw_qp_rules(struct
 	}
 	mutex_unlock(&qp->mc_flows_list.lock);
 }
+#endif
 
 static void destroy_raw_qp(struct mlx5_ib_dev *dev, struct mlx5_ib_qp *qp)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	destroy_raw_qp_rules(dev, qp);
+#endif
 
 	if (qp->rq.wqe_cnt) {
 		destroy_raw_qp_tir(dev, qp);
@@ -3970,7 +3976,11 @@ static void dump_wqe(struct mlx5_ib_qp *
 static void mlx5_bf_copy(u64 __iomem *dst, u64 *src,
 			 unsigned bytecnt, struct mlx5_ib_qp *qp)
 {
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	int i;
+#endif
 	while (bytecnt > 0) {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		__iowrite64_copy(dst++, src++, 8);
 		__iowrite64_copy(dst++, src++, 8);
 		__iowrite64_copy(dst++, src++, 8);
@@ -3979,6 +3989,13 @@ static void mlx5_bf_copy(u64 __iomem *ds
 		__iowrite64_copy(dst++, src++, 8);
 		__iowrite64_copy(dst++, src++, 8);
 		__iowrite64_copy(dst++, src++, 8);
+#else
+		i = 64;
+		while (i > 0) {
+			*dst++=*src++;
+			i--;
+		}
+#endif
 		bytecnt -= 64;
 		if (unlikely(src == qp->sq.qend))
 			src = mlx5_get_send_wqe(qp, 0);
--- a/drivers/infiniband/hw/mlx5/roce.c
+++ b/drivers/infiniband/hw/mlx5/roce.c
@@ -208,9 +208,11 @@ int mlx5_query_port_roce(struct ib_devic
 	props->state            = IB_PORT_DOWN;
 	props->phys_state       = 3;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (mlx5_query_nic_vport_qkey_viol_cntr(dev->mdev,
 						(u16 *)&props->qkey_viol_cntr))
 		pr_warn("%s failed to query qkey violations counter\n", __func__);
+#endif
 
 
 	if (!netdev)
--- a/drivers/net/ethernet/mellanox/mlx5/core/Makefile
+++ b/drivers/net/ethernet/mellanox/mlx5/core/Makefile
@@ -4,8 +4,4 @@ obj-$(CONFIG_MLX5_CORE)		+= mlx5_core.o
 
 mlx5_core-y :=	main.o cmd.o debugfs.o fw.o eq.o uar.o pagealloc.o \
 		health.o mcg.o cq.o srq.o alloc.o qp.o port.o mr.o pd.o   \
-		mad.o wq.o vport.o transobj.o en_main.o \
-		en_ethtool.o en_tx.o en_rx.o en_txrx.o \
-		sriov.o params.o en_selftest.o fs_cmd.o fs_core.o en_fs.o \
-		en_sniffer.o rl.o
-mlx5_core-$(CONFIG_RFS_ACCEL) +=  en_arfs.o
+		mad.o wq.o transobj.o params.o vport.o fs_cmd.o fs_core.o rl.o
--- a/drivers/net/ethernet/mellanox/mlx5/core/alloc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/alloc.c
@@ -58,7 +58,11 @@ static void *mlx5_dma_zalloc_coherent_no
 	mutex_lock(&priv->alloc_mutex);
 	original_node = dev_to_node(&dev->pdev->dev);
 	set_dev_node(&dev->pdev->dev, node);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	cpu_handle = dma_zalloc_coherent(&dev->pdev->dev, size,
+#else
+	cpu_handle = dma_alloc_coherent(&dev->pdev->dev, size,
+#endif
 					 dma_handle, GFP_KERNEL);
 	set_dev_node(&dev->pdev->dev, original_node);
 	mutex_unlock(&priv->alloc_mutex);
@@ -93,6 +97,10 @@ int mlx5_buf_alloc_node(struct mlx5_core
 			--buf->page_shift;
 			buf->npages *= 2;
 		}
+
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+		memset(buf->direct.buf, 0, size);
+#endif
 	} else {
 		int i;
 
@@ -114,6 +122,9 @@ int mlx5_buf_alloc_node(struct mlx5_core
 				goto err_free;
 
 			buf->page_list[i].map = t;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+			memset(buf->page_list[i].buf, 0, PAGE_SIZE);
+#endif
 		}
 
 		if (BITS_PER_LONG == 64) {
--- a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c
@@ -30,7 +30,9 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <asm-generic/kmap_types.h>
+#endif
 #include <linux/module.h>
 #include <linux/errno.h>
 #include <linux/pci.h>
@@ -38,7 +40,9 @@
 #include <linux/slab.h>
 #include <linux/delay.h>
 #include <linux/random.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/io-mapping.h>
+#endif
 #include <linux/mlx5/driver.h>
 #include <linux/debugfs.h>
 #include <linux/sysfs.h>
@@ -1041,7 +1045,11 @@ static ssize_t dbg_write(struct file *fi
 
 static const struct file_operations fops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.write	= dbg_write,
 };
 #endif
@@ -1261,7 +1269,11 @@ static ssize_t data_read(struct file *fi
 
 static const struct file_operations dfops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.write	= data_write,
 	.read	= data_read,
 };
@@ -1329,7 +1341,11 @@ static ssize_t outlen_write(struct file
 
 static const struct file_operations olfops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.write	= outlen_write,
 	.read	= outlen_read,
 };
--- a/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/debugfs.c
@@ -185,6 +185,10 @@ static ssize_t average_read(struct file
 {
 	struct mlx5_cmd_stats *stats;
 	u64 field = 0;
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+	u64 dividend;
+	u32 divisor;
+#endif
 	int ret;
 	char tbuf[22];
 
@@ -194,7 +198,14 @@ static ssize_t average_read(struct file
 	stats = filp->private_data;
 	spin_lock_irq(&stats->lock);
 	if (stats->n)
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		field = div64_u64(stats->sum, stats->n);
+#else
+		dividend = stats->sum;
+		divisor = stats->n;
+		do_div(dividend, divisor);
+		field = dividend;
+#endif
 	spin_unlock_irq(&stats->lock);
 	ret = snprintf(tbuf, sizeof(tbuf), "%llu\n", field);
 	if (ret > 0) {
@@ -267,7 +278,11 @@ static ssize_t average_write(struct file
 
 static const struct file_operations stats_fops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.read	= average_read,
 	.write	= average_write,
 };
@@ -608,7 +623,11 @@ static ssize_t dbg_read(struct file *fil
 
 static const struct file_operations fops = {
 	.owner	= THIS_MODULE,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	.open	= simple_open,
+#else
+	.open	= compat_mlx5_simple_open,
+#endif
 	.read	= dbg_read,
 };
 
--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c
@@ -30,14 +30,18 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <asm-generic/kmap_types.h>
+#endif
 #include <linux/module.h>
 #include <linux/init.h>
 #include <linux/errno.h>
 #include <linux/pci.h>
 #include <linux/dma-mapping.h>
 #include <linux/slab.h>
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <linux/io-mapping.h>
+#endif
 #include <linux/interrupt.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/cq.h>
@@ -459,6 +463,7 @@ static struct mlx5_profile profile[] = {
 			.size	= 64,
 			.limit	= 32
 		},
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 		.mr_cache[13]	= {
 			.size	= 32,
 			.limit	= 16
@@ -467,6 +472,7 @@ static struct mlx5_profile profile[] = {
 			.size	= 16,
 			.limit	= 8
 		},
+#endif
 	},
 };
 
@@ -586,7 +592,7 @@ static int mlx5_enable_msix(struct mlx5_
 	int nvec;
 #ifndef HAVE_PCI_ENABLE_MSIX_RANGE
 	int err;
-#endif 
+#endif
 	int i;
 
 #if LINUX_VERSION_CODE < KERNEL_VERSION(2, 6, 19)
@@ -943,6 +949,7 @@ int mlx5_core_disable_hca(struct mlx5_co
 	return mlx5_cmd_exec_check_status(dev, in, sizeof(in), out, sizeof(out));
 }
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 static u32 internal_timer_h(struct mlx5_core_dev *dev)
 {
 	return ioread32be(&dev->iseg->internal_timer_h);
@@ -974,6 +981,26 @@ ret:
 	return (u64)timer_l | (u64)timer_h1 << 32;
 }
 EXPORT_SYMBOL(mlx5_core_read_clock);
+#endif
+
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+static void compat_pci_set_master(struct pci_dev *dev, bool enable)
+{
+	u16 old_cmd, cmd;
+
+	pci_read_config_word(dev, PCI_COMMAND, &old_cmd);
+	if (enable)
+		cmd = old_cmd | PCI_COMMAND_MASTER;
+	else
+		cmd = old_cmd & ~PCI_COMMAND_MASTER;
+	if (cmd != old_cmd) {
+		dev_dbg(&dev->dev, "%s bus mastering\n",
+			enable ? "enabling" : "disabling");
+		pci_write_config_word(dev, PCI_COMMAND, cmd);
+	}
+	dev->is_busmaster = enable;
+}
+#endif
 
 static int mlx5_core_set_issi(struct mlx5_core_dev *dev)
 {
@@ -1539,6 +1566,8 @@ static int mlx5_pci_init(struct mlx5_cor
 err_clr_master:
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 29)
 	pci_clear_master(dev->pdev);
+#else
+	compat_pci_set_master(dev->pdev, true);
 #endif
 	release_bar(dev->pdev);
 err_disable:
@@ -1554,6 +1583,8 @@ static void mlx5_pci_close(struct mlx5_c
 	iounmap(dev->iseg);
 #if LINUX_VERSION_CODE >= KERNEL_VERSION(2, 6, 29)
 	pci_clear_master(dev->pdev);
+#else
+	compat_pci_set_master(dev->pdev, true);
 #endif
 	release_bar(dev->pdev);
 	mlx5_pci_disable_device(dev);
@@ -1863,11 +1894,13 @@ static int mlx5_load_one(struct mlx5_cor
 	mlx5_init_mr_table(dev);
 	mlx5_init_dct_table(dev);
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	err = mlx5_init_fs(dev);
 	if (err) {
 		mlx5_core_err(dev, "flow steering init %d\n", err);
 		goto err_reg_dev;
 	}
+#endif
 
 	err = mlx5_init_rl_table(dev);
 	if (err) {
@@ -1917,7 +1950,9 @@ err_rl:
 #endif
 	mlx5_cleanup_rl_table(dev);
 err_fs:
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mlx5_cleanup_fs(dev);
+#endif
 err_reg_dev:
 	mlx5_cleanup_dct_table(dev);
 	mlx5_cleanup_mr_table(dev);
@@ -1993,7 +2028,9 @@ static int mlx5_unload_one(struct mlx5_c
 	mlx5_eswitch_cleanup(dev->priv.eswitch);
 #endif
 	mlx5_cleanup_rl_table(dev);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mlx5_cleanup_fs(dev);
+#endif
 	mlx5_cleanup_dct_table(dev);
 	mlx5_cleanup_mr_table(dev);
 	mlx5_cleanup_srq_table(dev);
@@ -2471,7 +2508,9 @@ static int __init init(void)
 	if (err)
 		goto err_debug;
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mlx5e_init();
+#endif
 
 	return 0;
 
@@ -2484,7 +2523,9 @@ err_debug:
 
 static void __exit cleanup(void)
 {
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	mlx5e_cleanup();
+#endif
 	pci_unregister_driver(&mlx5_core_driver);
 #ifndef HAVE_NO_DEBUGFS
 	mlx5_unregister_debugfs();
--- a/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/mlx5_core.h
@@ -133,6 +133,15 @@ u32 mlx5_get_msix_vec(struct mlx5_core_d
 void mlx5e_init(void);
 void mlx5e_cleanup(void);
 
+#if (LINUX_VERSION_CODE <= KERNEL_VERSION(2, 6, 16))
+static inline int compat_mlx5_simple_open(struct inode *inode, struct file *file)
+{
+	file->private_data = inode->i_private;
+
+	return 0;
+}
+#endif
+
 /*Sniffer callback for RoCE rules*/
 enum roce_action {
 	ROCE_ON,
--- a/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/pagealloc.c
@@ -30,7 +30,11 @@
  * SOFTWARE.
  */
 
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 #include <asm-generic/kmap_types.h>
+#else
+#include <linux/vmalloc.h>
+#endif
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/mlx5/driver.h>
--- a/drivers/net/ethernet/mellanox/mlx5/core/uar.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/uar.c
@@ -32,7 +32,9 @@
 
 #include <linux/kernel.h>
 #include <linux/module.h>
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16)
 #include <linux/io-mapping.h>
+#endif
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/cmd.h>
 #include "mlx5_core.h"
--- a/drivers/net/ethernet/mellanox/mlx5/core/vport.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/vport.c
@@ -627,13 +627,14 @@ static int mlx5_nic_vport_enable_disable
 	err = mlx5_modify_nic_vport_context(mdev, in, inlen);
 
 	kvfree(in);
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	if (!err) {
 		if (enable_disable)
 			mlx5e_sniffer_roce_mode_notify(mdev, ROCE_ON);
 		else
 			mlx5e_sniffer_roce_mode_notify(mdev, ROCE_OFF);
 	}
-
+#endif
 	return err;
 }
 
--- a/include/linux/clocksource.h
+++ b/include/linux/clocksource.h
@@ -1,8 +1,9 @@
 #ifndef LINUX_CLOCKSOURCE_H
 #define LINUX_CLOCKSOURCE_H
 
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16)
 #include_next <linux/clocksource.h>
-
+#endif
 #if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 #ifndef HAVE_TIMECOUNTER_ADJTIME
 /**
--- a/include/linux/mlx5/driver.h
+++ b/include/linux/mlx5/driver.h
@@ -694,7 +694,11 @@ struct mlx5_core_dev {
 	u32 *hca_caps_cur[MLX5_CAP_NUM];
 	u32 *hca_caps_max[MLX5_CAP_NUM];
 #endif
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
 	phys_addr_t		iseg_base;
+#else
+	u64			iseg_base;
+#endif
 	struct mlx5_init_seg __iomem *iseg;
 	enum mlx5_device_state	state;
 	struct mutex		intf_state_mutex;
@@ -1015,7 +1019,9 @@ int mlx5_satisfy_startup_pages(struct ml
 int mlx5_reclaim_startup_pages(struct mlx5_core_dev *dev);
 void mlx5_register_debugfs(void);
 void mlx5_unregister_debugfs(void);
+#if LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 18)
 cycle_t mlx5_core_read_clock(struct mlx5_core_dev *dev);
+#endif
 int mlx5_eq_init(struct mlx5_core_dev *dev);
 void mlx5_eq_cleanup(struct mlx5_core_dev *dev);
 void mlx5_fill_page_array(struct mlx5_buf *buf, __be64 *pas);
@@ -1178,7 +1184,11 @@ enum {
 };
 
 enum {
-	MAX_MR_CACHE_ENTRIES    = 15,
+#if (LINUX_VERSION_CODE > KERNEL_VERSION(2, 6, 16))
+       MAX_MR_CACHE_ENTRIES    = 15,
+#else
+	MAX_MR_CACHE_ENTRIES    = 13,
+#endif
 };
 
 struct mlx5_interface {
